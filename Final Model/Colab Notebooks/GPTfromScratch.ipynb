{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUY2TylL6kouocE681puiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/SmallLanguageModel-project/blob/main/Final%20Model/Colab%20Notebooks/GPTfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WQ820NsVprM",
        "outputId": "43d479a8-ae89-4412-e734-b35da716b929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.11.17)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.1\n"
          ]
        }
      ],
      "source": [
        "# run this first always!\n",
        "!pip install python-dotenv\n",
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channel_Id_Json  = [\n",
        "  \"UCb_MAhL8Thb3HJ_wPkH3gcw\", #phil edwards\n",
        "  \"UCA295QVkf9O1RQ8_-s3FVXg\", #aevy tv\n",
        "  \"UCpFFItkfZz1qz5PpHpqzYBw\", #nexpo\n",
        "  \"UCY1kMZp36IQSyNx_9h4mpCg\", #mark robber\n",
        "  \"UCA19mAJURyYHbJzhfpqhpCA\", #action lab shorts\n",
        "  \"UCqnbDFdCpuN8CMEg0VuEBqA\", #new york times\n",
        "  \"UCddiUEpeqJcYeBxX1IVBKvQ\", #the verge\n",
        "  \"UCcefcZRL2oaA_uBNeo5UOWg\", #y-combinator\n",
        "  \"UCLXo7UDZvByw2ixzpQCufnA\", #vox\n",
        "  \"UCsQoiOrh7jzKmE8NBofhTnQ\", #varun mayya\n",
        "  \"UCUyvQV2JsICeLZP4c_h40kA\", #thomas flight\n",
        "  \"UCvjgXvBlbQiydffZU7m1_aw\", #the coding train\n",
        "  \"UCRI00CwLZdLRCWg5BdDOsNw\", #canadian lad\n",
        "  \"UCEIwxahdLz7bap-VDs9h35A\", #steve mould\n",
        "  \"UC4bq21IPPbpu0Qrsl7LW0sw\", #slidebean\n",
        "  \"UCR1IuLEqb6UEA_zQ81kwXfg\", #real engineering\n",
        "  \"UCIlU5KDHKFSaebYviKfOidw\", #newsthink\n",
        "  \"UCtYKe7-XbaDjpUwcU5x0bLg\", #neo\n",
        "  \"UCBJycsmduvYEL83R_U4JriQ\", #mkbdh\n",
        "  \"UCRcgy6GzDeccI7dkbbBna3Q\", #lemmino\n",
        "  \"UC3_BakzLfadvFrsnClMFWmQ\", #john coogan\n",
        "  \"UCmGSJVG3mCRXVOP4yZrU1Dw\", #johnny harris\n",
        "  \"UCFN6lQpfY8XIRdhv9G-f4bg\", #henry belcaster\n",
        "  \"UConJDkGk921yT9hISzFqpzw\", #freethink\n",
        "  \"UClWTCPVi-AU9TeCN6FkGARg\", #EO\n",
        "  \"UCyHJ94JzwY92NsBVzJ2aE3Q\", #econ\n",
        "  \"UCTqEu1wZDBju2tHkNP1dwzQ\", #earthrise\n",
        "  \"UCcabW7890RKJzL968QWEykA\", #CS 50\n",
        "  \"UCamLstJyCa-t5gfZegxsFMw\", #colin and samir\n",
        "  \"UC415bOPUcGSamy543abLmRA\", #cleo abraham\n",
        "  \"UCpMcsdZf2KkAnfmxiq2MfMQ\", #arvin ash\n",
        "  \"UCqVEHtQoXHmUCfJ-9smpTSg\", #answer in progress\n",
        "  \"UCYO_jab_esuFRV4b17AJtAw\", #3blue1brown\n",
        "  \"UCHnyfMqiRRG1u-2MsSQLbXA\", #veritasium\n",
        "  \"UCsXVk37bltHxD1rDPwtNM8Q\", #kurzgesagt\n",
        "  \"UC9RM-iSvTu1uPJb8X5yp3EQ\" #wendover\n",
        "]"
      ],
      "metadata": {
        "id": "lnb6CX6CYH6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv('yt_secret_key')"
      ],
      "metadata": {
        "id": "IM6sarwEf7_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import TranscriptsDisabled, YouTubeTranscriptApi\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='youtube_fetch.log', level=logging.ERROR)\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)"
      ],
      "metadata": {
        "id": "IUvSjNgzf_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "videoNo = 0\n",
        "for links in channel_Id_Json:\n",
        "  next_page_token = None\n",
        "  videoIds = []\n",
        "\n",
        "  while True:\n",
        "    channelRes = youtube.channels().list(\n",
        "      part='contentDetails', id=links\n",
        "    ).execute()\n",
        "\n",
        "    if 'items' in channelRes and channelRes['items']:\n",
        "      playlistId = channelRes['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "      playlistResult = youtube.playlistItems().list(\n",
        "        part='contentDetails', playlistId=playlistId,\n",
        "        maxResults = 100, pageToken = next_page_token\n",
        "      ).execute()\n",
        "\n",
        "      videoIds.extend([item['contentDetails']['videoId'] for item in playlistResult.get('items', [])])\n",
        "\n",
        "      next_page_token = playlistResult.get('nextPageToken')\n",
        "\n",
        "      if not next_page_token:\n",
        "        break\n",
        "\n",
        "  for ids in videoIds:\n",
        "    videoUrl = f\"https://www.youtube.com/watch?v={ids}\"\n",
        "    try:\n",
        "      raw_transcripts = []\n",
        "      try:\n",
        "        captions = YouTubeTranscriptApi.get_transcript(\n",
        "          ids, languages=['en'], preserve_formatting=True\n",
        "        )\n",
        "        if captions:\n",
        "          formatted_captions = [{'text': caption['text']} for caption in captions]\n",
        "          raw_transcripts.append(formatted_captions)\n",
        "          videoNo += 1\n",
        "          print(f\"Number of videos with valid captions are: {videoNo}\")\n",
        "        else:\n",
        "          continue\n",
        "      except TranscriptsDisabled as e:\n",
        "        print(F\"There was an error while getting the captions: {e}\")\n",
        "      except Exception as e:\n",
        "        logging.error(f\"There was some error while fetching the video: {str(e)}\")\n",
        "    except Exception as e:\n",
        "      logging.error(f\"There was some error while getting the captions: {str(e)}\")\n",
        "\n",
        "    with open('training_data.txt', 'a', encoding='utf-8') as file:\n",
        "      for videoCaptions in raw_transcripts:\n",
        "        for line in videoCaptions:\n",
        "          file.write(line['text'] + ' ')\n",
        "\n",
        "print(f\"time taken to execute the code is {(timeit.default_timer() - start_time) / 60} mins\")"
      ],
      "metadata": {
        "id": "fOhjed9EgCIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coding the GPT from here**"
      ],
      "metadata": {
        "id": "1ewiBOXRgSLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_data.txt', 'r', encoding='utf-8') as file:\n",
        "  text = file.read()"
      ],
      "metadata": {
        "id": "ABiq6op4gQLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the text data\n",
        "import re\n",
        "text = re.sub('[^a-zA-Z0-9\\s.,!?]', '', text)"
      ],
      "metadata": {
        "id": "vtLoIo4Lgzw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total no of chars and vocab size\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "UzUADm9lgwVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enoder - decoder\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "zaUU1Q-XgUag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the data + train-test split\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "6-1s7eqKhGLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "257tpo6jhXtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}