{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCm6cDXTkmZXOVKK16uIRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/SmallLanguageModel-project/blob/main/Demo%20Models/Colab%20Notebooks/DemoRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "Eooeb4JziRNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = list(uploaded_files.keys())[0]\n",
        "\n",
        "# Read the uploaded file into a DataFrame\n",
        "data = pd.read_csv(file_name)"
      ],
      "metadata": {
        "id": "AN8MZc8Ri9D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract values as a NumPy array\n",
        "tfidf_matrix = data.values.T\n",
        "\n",
        "# Assuming tfidf_matrix is your TF-IDF matrix\n",
        "reference_document_index = 0  # Choose a reference document\n",
        "reference_vector = tfidf_matrix[:, reference_document_index].reshape(1, -1)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "similarity_scores = cosine_similarity(tfidf_matrix.T, reference_vector)\n",
        "\n",
        "# Normalize similarity scores to [0, 1]\n",
        "normalized_scores = (similarity_scores - np.min(similarity_scores)) / (np.max(similarity_scores) - np.min(similarity_scores))\n",
        "\n",
        "# Set target values\n",
        "your_target_values_for_each_document = normalized_scores.flatten()\n",
        "\n",
        "# Print normalized scores\n",
        "print(\"Normalized Similarity Scores:\")\n",
        "print(your_target_values_for_each_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP9xJwPFiLqC",
        "outputId": "b073552d-87a9-4ac0-8bb5-8bae748625eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Similarity Scores:\n",
            "[nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-64a7b7cf2052>:12: RuntimeWarning: invalid value encountered in divide\n",
            "  normalized_scores = (similarity_scores - np.min(similarity_scores)) / (np.max(similarity_scores) - np.min(similarity_scores))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define the RNN class\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.W_ih = np.random.randn(hidden_size, input_size)\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size)\n",
        "        self.W_ho = np.random.randn(output_size, hidden_size)\n",
        "\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_o = np.zeros((output_size, 1))\n",
        "\n",
        "        # Initialize hidden state\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the RNN\n",
        "        self.h = sigmoid(np.dot(self.W_ih, x) + np.dot(self.W_hh, self.h) + self.b_h)\n",
        "        output = sigmoid(np.dot(self.W_ho, self.h) + self.b_o)\n",
        "        return output\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        # Backward pass through the RNN\n",
        "        error = y - output\n",
        "\n",
        "        # Output layer gradients\n",
        "        d_output = error * sigmoid_derivative(output)\n",
        "        d_W_ho = np.dot(d_output, self.h.T)\n",
        "        d_b_o = d_output\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        d_h = np.dot(self.W_ho.T, d_output) * sigmoid_derivative(self.h)\n",
        "        d_W_ih = np.dot(d_h, x.T)\n",
        "        d_W_hh = np.dot(d_h, self.h.T)\n",
        "        d_b_h = d_h\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W_ih += learning_rate * d_W_ih\n",
        "        self.W_hh += learning_rate * d_W_hh\n",
        "        self.W_ho += learning_rate * d_W_ho\n",
        "        self.b_h += learning_rate * d_b_h\n",
        "        self.b_o += learning_rate * d_b_o\n",
        "\n",
        "# Load TF-IDF vectors\n",
        "tfidf_matrix = np.loadtxt('vector_data.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "# Transpose the TF-IDF matrix to have a shape of (number_of_features, number_of_documents)\n",
        "tfidf_matrix = tfidf_matrix.T\n",
        "\n",
        "# Reshape TF-IDF vectors to have a shape of (number_of_features, 1)\n",
        "tfidf_matrix = tfidf_matrix.reshape(tfidf_matrix.shape[0], 1)\n",
        "\n",
        "# Example usage\n",
        "input_size = tfidf_matrix.shape[0]\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "# Create RNN\n",
        "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(tfidf_matrix.shape[1]):\n",
        "        x = tfidf_matrix[:, i].reshape(-1, 1)\n",
        "        y = np.array([[your_target_values_for_each_document[i]]])  # Replace with your target values\n",
        "\n",
        "        # Forward pass\n",
        "        output = rnn.forward(x)\n",
        "\n",
        "        # Backward pass\n",
        "        rnn.backward(x, y, output, learning_rate)\n",
        "\n",
        "        # Compute loss (mean squared error)\n",
        "        total_loss += np.mean((output - y) ** 2)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss}')\n",
        "\n",
        "# Test the trained model\n",
        "for i in range(tfidf_matrix.shape[1]):\n",
        "    x = tfidf_matrix[:, i].reshape(-1, 1)\n",
        "    output = rnn.forward(x)\n",
        "    print(f'Input: {x.flatten()}, Output: {output.flatten()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJzJb2Ixjjza",
        "outputId": "c46b9ffa-5487-4c2d-e03c-fe2c7a572fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: nan\n",
            "Epoch 1000, Loss: nan\n",
            "Epoch 2000, Loss: nan\n",
            "Epoch 3000, Loss: nan\n",
            "Epoch 4000, Loss: nan\n",
            "Epoch 5000, Loss: nan\n",
            "Epoch 6000, Loss: nan\n",
            "Epoch 7000, Loss: nan\n",
            "Epoch 8000, Loss: nan\n",
            "Epoch 9000, Loss: nan\n",
            "Input: [0.00000000e+00 8.61443503e-06 1.29216525e-05 ... 4.30721752e-06\n",
            " 4.30721752e-06 4.30721752e-06], Output: [nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "data = pd.read_csv('vector_data.csv')\n",
        "\n",
        "# Extract values as a NumPy array\n",
        "tfidf_matrix = data.values.T\n",
        "\n",
        "# Assuming tfidf_matrix is your TF-IDF matrix\n",
        "reference_document_index = 0\n",
        "reference_vector = tfidf_matrix[:, reference_document_index].reshape(1, -1)\n",
        "\n",
        "# Compute cosine similarity scores\n",
        "similarity_scores = cosine_similarity(tfidf_matrix.T, reference_vector)\n",
        "\n",
        "# Normalize similarity scores to [0, 1]\n",
        "normalized_scores = (similarity_scores - np.min(similarity_scores)) / (np.max(similarity_scores) - np.min(similarity_scores))\n",
        "\n",
        "# Set target values\n",
        "your_target_values_for_each_document = normalized_scores.flatten()\n",
        "\n",
        "# Print normalized scores\n",
        "print(\"Normalized Similarity Scores:\")\n",
        "print(your_target_values_for_each_document)\n",
        "\n",
        "# Define sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define the RNN class\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.W_ih = np.random.randn(hidden_size, input_size)\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size)\n",
        "        self.W_ho = np.random.randn(output_size, hidden_size)\n",
        "\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_o = np.zeros((output_size, 1))\n",
        "\n",
        "        # Initialize hidden state\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the RNN\n",
        "        self.h = sigmoid(np.dot(self.W_ih, x) + np.dot(self.W_hh, self.h) + self.b_h)\n",
        "        output = sigmoid(np.dot(self.W_ho, self.h) + self.b_o)\n",
        "        return output\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        # Backward pass through the RNN\n",
        "        error = y - output\n",
        "\n",
        "        # Output layer gradients\n",
        "        d_output = error * sigmoid_derivative(output)\n",
        "        d_W_ho = np.dot(d_output, self.h.T)\n",
        "        d_b_o = d_output\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        d_h = np.dot(self.W_ho.T, d_output) * sigmoid_derivative(self.h)\n",
        "        d_W_ih = np.dot(d_h, x.T)\n",
        "        d_W_hh = np.dot(d_h, self.h.T)\n",
        "        d_b_h = d_h\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W_ih += learning_rate * d_W_ih\n",
        "        self.W_hh += learning_rate * d_W_hh\n",
        "        self.W_ho += learning_rate * d_W_ho\n",
        "        self.b_h += learning_rate * d_b_h\n",
        "        self.b_o += learning_rate * d_b_o\n",
        "\n",
        "    def generate(self, seed, num_tokens):\n",
        "        # Generate tokens using the trained RNN\n",
        "        generated_sequence = []\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        # Use the seed as the first input\n",
        "        x = seed.reshape(-1, 1)\n",
        "\n",
        "        for _ in range(num_tokens):\n",
        "            # Forward pass through the RNN\n",
        "            h = sigmoid(np.dot(self.W_ih, x) + np.dot(self.W_hh, h) + self.b_h)\n",
        "            output = sigmoid(np.dot(self.W_ho, h) + self.b_o)\n",
        "\n",
        "            # Append the generated token to the sequence\n",
        "            generated_sequence.append(output.flatten())\n",
        "\n",
        "            # Use the generated output as the input for the next time step\n",
        "            x = output\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "# Load TF-IDF vectors\n",
        "tfidf_matrix = np.loadtxt('vector_data.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "# Transpose the TF-IDF matrix to have a shape of (number_of_features, number_of_documents)\n",
        "tfidf_matrix = tfidf_matrix.T\n",
        "\n",
        "# Reshape TF-IDF vectors to have a shape of (number_of_features, 1)\n",
        "tfidf_matrix = tfidf_matrix.reshape(tfidf_matrix.shape[0], 1)\n",
        "\n",
        "# Example usage\n",
        "input_size = tfidf_matrix.shape[0]\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "# Create RNN\n",
        "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(tfidf_matrix.shape[1]):\n",
        "        x = tfidf_matrix[:, i].reshape(-1, 1)\n",
        "        y = np.array([[your_target_values_for_each_document[i]]])  # Replace with your target values\n",
        "\n",
        "        # Forward pass\n",
        "        output = rnn.forward(x)\n",
        "\n",
        "        # Backward pass\n",
        "        rnn.backward(x, y, output, learning_rate)\n",
        "\n",
        "        # Compute loss (mean squared error)\n",
        "        total_loss += np.mean((output - y) ** 2)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss}')\n",
        "\n",
        "# # Test the trained model\n",
        "# for i in range(tfidf_matrix.shape[1]):\n",
        "#     x = tfidf_matrix[:, i].reshape(-1, 1)\n",
        "#     output = rnn.forward(x)\n",
        "#     # print(f'Input: {x.flatten()}, Output: {output.flatten()}')\n",
        "\n",
        "# seed_token = tfidf_matrix[:, 0].reshape(-1, 1)\n",
        "# generated_tokens = rnn.generate(seed_token, num_tokens=4)\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer as vect\n",
        "\n",
        "# # Inverse TF-IDF transform\n",
        "# inverse_tfidf_matrix = vect.inverse_transform(generated_tokens)\n",
        "\n",
        "# # Convert the TF vectors back to text\n",
        "# reconstructed_text_data = [' '.join(words) for words in inverse_tfidf_matrix]\n",
        "\n",
        "# # Print the reconstructed text data\n",
        "# print(\"Reconstructed Text Data:\")\n",
        "# print(reconstructed_text_data)\n",
        "\n",
        "# Test the trained model\n",
        "for i in range(tfidf_matrix.shape[1]):\n",
        "    x = tfidf_matrix[:, i].reshape(-1, 1)\n",
        "    output = rnn.forward(x)\n",
        "    # print(f'Input: {x.flatten()}, Output: {output.flatten()}')\n",
        "\n",
        "seed_token = tfidf_matrix[:, 0].reshape(-1, 1)\n",
        "num_generated_tokens = 4\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as vect\n",
        "\n",
        "# Generate tokens using the trained RNN\n",
        "generated_tokens = rnn.generate(seed_token, num_generated_tokens)\n",
        "\n",
        "# Convert the generated tokens back to text using the original TF-IDF vectorizer\n",
        "inverse_tfidf_matrix = vect.inverse_transform(generated_tokens)\n",
        "\n",
        "# Convert the TF vectors back to text\n",
        "reconstructed_text_data = [' '.join(words) for words in inverse_tfidf_matrix]\n",
        "\n",
        "# Print the reconstructed text data\n",
        "print(\"Reconstructed Text Data:\")\n",
        "print(reconstructed_text_data)"
      ],
      "metadata": {
        "id": "6JXnufLp2g5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTbNTK904oHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}