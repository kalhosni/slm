{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GirIh-L3I0luR95oxwMP6x5nbwmPS7yk",
      "authorship_tag": "ABX9TyMrvOnRpzl5rFbVClNkyvn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/SmallLanguageModel-project/blob/main/Demo%20Models/Colab%20Notebooks/GPTfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WQ820NsVprM"
      },
      "outputs": [],
      "source": [
        "# run this first always!\n",
        "!pip install python-dotenv\n",
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()"
      ],
      "metadata": {
        "id": "KFpOTb38uANS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_Id_Json  = [\n",
        "  \"UCb_MAhL8Thb3HJ_wPkH3gcw\", #phil edwards\n",
        "  \"UCA295QVkf9O1RQ8_-s3FVXg\", #aevy tv\n",
        "  \"UCpFFItkfZz1qz5PpHpqzYBw\", #nexpo\n",
        "  \"UCY1kMZp36IQSyNx_9h4mpCg\", #mark robber\n",
        "  \"UCA19mAJURyYHbJzhfpqhpCA\", #action lab shorts\n",
        "  \"UCqnbDFdCpuN8CMEg0VuEBqA\", #new york times\n",
        "  \"UCddiUEpeqJcYeBxX1IVBKvQ\", #the verge\n",
        "  \"UCcefcZRL2oaA_uBNeo5UOWg\", #y-combinator\n",
        "  \"UCLXo7UDZvByw2ixzpQCufnA\", #vox\n",
        "  \"UCsQoiOrh7jzKmE8NBofhTnQ\", #varun mayya\n",
        "  \"UCUyvQV2JsICeLZP4c_h40kA\", #thomas flight\n",
        "  \"UCvjgXvBlbQiydffZU7m1_aw\", #the coding train\n",
        "  \"UCRI00CwLZdLRCWg5BdDOsNw\", #canadian lad\n",
        "  \"UCEIwxahdLz7bap-VDs9h35A\", #steve mould\n",
        "  \"UC4bq21IPPbpu0Qrsl7LW0sw\", #slidebean\n",
        "  \"UCR1IuLEqb6UEA_zQ81kwXfg\", #real engineering\n",
        "  \"UCIlU5KDHKFSaebYviKfOidw\", #newsthink\n",
        "  \"UCtYKe7-XbaDjpUwcU5x0bLg\", #neo\n",
        "  \"UCBJycsmduvYEL83R_U4JriQ\", #mkbdh\n",
        "  \"UCRcgy6GzDeccI7dkbbBna3Q\", #lemmino\n",
        "  \"UC3_BakzLfadvFrsnClMFWmQ\", #john coogan\n",
        "  \"UCmGSJVG3mCRXVOP4yZrU1Dw\", #johnny harris\n",
        "  \"UCFN6lQpfY8XIRdhv9G-f4bg\", #henry belcaster\n",
        "  \"UConJDkGk921yT9hISzFqpzw\", #freethink\n",
        "  \"UClWTCPVi-AU9TeCN6FkGARg\", #EO\n",
        "  \"UCyHJ94JzwY92NsBVzJ2aE3Q\", #econ\n",
        "  \"UCTqEu1wZDBju2tHkNP1dwzQ\", #earthrise\n",
        "  \"UCcabW7890RKJzL968QWEykA\", #CS 50\n",
        "  \"UCamLstJyCa-t5gfZegxsFMw\", #colin and samir\n",
        "  \"UC415bOPUcGSamy543abLmRA\", #cleo abraham\n",
        "  \"UCpMcsdZf2KkAnfmxiq2MfMQ\", #arvin ash\n",
        "  \"UCqVEHtQoXHmUCfJ-9smpTSg\", #answer in progress\n",
        "  \"UCYO_jab_esuFRV4b17AJtAw\", #3blue1brown\n",
        "  \"UCHnyfMqiRRG1u-2MsSQLbXA\", #veritasium\n",
        "  \"UCsXVk37bltHxD1rDPwtNM8Q\", #kurzgesagt\n",
        "  \"UC9RM-iSvTu1uPJb8X5yp3EQ\", #wendover\n",
        "  \"UCZaT_X_mc0BI-djXOlfhqWQ\", #vice news\n",
        "  \"UCMiJRAwDNSNzuYeN2uWa0pA\", #mrwhosetheboss\n",
        "  \"UCHpw8xwDNhU9gdohEcJu4aA\", #theguardian\n",
        "  \"UCK7tptUDHh-RYDsdxO1-5QQ\", #wallstreetjournal\n",
        "  \"UCsooa4yRKGN_zEE8iknghZA\", #ted-ed\n",
        "]"
      ],
      "metadata": {
        "id": "lnb6CX6CYH6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv('yt_secret_key')"
      ],
      "metadata": {
        "id": "IM6sarwEf7_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import TranscriptsDisabled, YouTubeTranscriptApi\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='youtube_fetch.log', level=logging.ERROR)\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)"
      ],
      "metadata": {
        "id": "IUvSjNgzf_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "videoNo = 0\n",
        "for links in channel_Id_Json:\n",
        "  next_page_token = None\n",
        "  videoIds = []\n",
        "\n",
        "  while True:\n",
        "    channelRes = youtube.channels().list(\n",
        "      part='contentDetails', id=links\n",
        "    ).execute()\n",
        "\n",
        "    if 'items' in channelRes and channelRes['items']:\n",
        "      playlistId = channelRes['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "      playlistResult = youtube.playlistItems().list(\n",
        "        part='contentDetails', playlistId=playlistId,\n",
        "        maxResults = 100, pageToken = next_page_token\n",
        "      ).execute()\n",
        "\n",
        "      videoIds.extend([item['contentDetails']['videoId'] for item in playlistResult.get('items', [])])\n",
        "\n",
        "      next_page_token = playlistResult.get('nextPageToken')\n",
        "\n",
        "      if not next_page_token:\n",
        "        break\n",
        "\n",
        "  for ids in videoIds:\n",
        "    videoUrl = f\"https://www.youtube.com/watch?v={ids}\"\n",
        "    try:\n",
        "      raw_transcripts = []\n",
        "      try:\n",
        "        captions = YouTubeTranscriptApi.get_transcript(\n",
        "          ids, languages=['en'], preserve_formatting=True\n",
        "        )\n",
        "        if captions:\n",
        "          formatted_captions = [{'text': caption['text']} for caption in captions]\n",
        "          raw_transcripts.append(formatted_captions)\n",
        "          videoNo += 1\n",
        "          print(f\"Number of videos with valid captions are: {videoNo}\")\n",
        "        else:\n",
        "          continue\n",
        "      except TranscriptsDisabled as e:\n",
        "        print(F\"There was an error while getting the captions: {e}\")\n",
        "      except Exception as e:\n",
        "        logging.error(f\"There was some error while fetching the video: {str(e)}\")\n",
        "    except Exception as e:\n",
        "      logging.error(f\"There was some error while getting the captions: {str(e)}\")\n",
        "\n",
        "    with open('new_training_data.txt', 'a', encoding='utf-8') as file:\n",
        "      for videoCaptions in raw_transcripts:\n",
        "        for line in videoCaptions:\n",
        "          file.write(line['text'] + ' ')\n",
        "\n",
        "print(f\"time taken to execute the code is {(timeit.default_timer() - start_time) / 60} mins\")"
      ],
      "metadata": {
        "id": "fOhjed9EgCIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_coll = timeit.default_timer()\n",
        "print(f\"time taken to fetch and write the data {(data_coll - start_time) / 3600} hrs\")"
      ],
      "metadata": {
        "id": "XF8Rx-eYuLcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**coding the tokenizer first**"
      ],
      "metadata": {
        "id": "1ewiBOXRgSLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nstQNTFXrWgJ",
        "outputId": "12f03120-4c44-4175-f09f-e631f247af93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "start_time_2 = timeit.default_timer()"
      ],
      "metadata": {
        "id": "YLGNbB0vltCY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data for training the BPE\n",
        "with open('training_data.txt', 'r', encoding='utf-8') as file:\n",
        "  captions = file.read()\n",
        "\n",
        "# tokenizing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "token_caps = nltk.word_tokenize(captions)"
      ],
      "metadata": {
        "id": "32uyvCyC7i8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dde4929-66f3-4586-d0d2-3573e294b7b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "n = int(0.8*len(token_caps))\n",
        "bpe_train_data = token_caps[:n]\n",
        "bpe_val_data = token_caps[n:]"
      ],
      "metadata": {
        "id": "fZuR1Kyw7qCp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a sub-word level tokenizer\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class SubwordTokenizer:\n",
        "  def __init__(self, num_merges):\n",
        "    self.num_merges = num_merges\n",
        "    self.vocab = None\n",
        "\n",
        "  def get_stats(self, vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "      symbols = word.split()\n",
        "      for i in range(len(symbols) - 1):\n",
        "        pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "  def merge_vocab(self, pair, vocab):\n",
        "    new_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in vocab:\n",
        "      new_word = word.replace(bigram, replacement)\n",
        "      new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "  def learn_bpe(self, data):\n",
        "    vocab = defaultdict(int)\n",
        "    for word in data:\n",
        "      vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    for i in range(self.num_merges):\n",
        "      pairs = self.get_stats(vocab)\n",
        "      if not pairs:\n",
        "        break\n",
        "      best_pair = max(pairs, key=pairs.get)\n",
        "      vocab = self.merge_vocab(best_pair, vocab)\n",
        "\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    tokens = []\n",
        "    if isinstance(text, list):  # check if text is a list\n",
        "      for word in text:\n",
        "        word = ' '.join(list(word)) + ' </w>'\n",
        "        while word:\n",
        "          if word in self.vocab:\n",
        "            tokens.append(word)\n",
        "            break\n",
        "          else:\n",
        "            tokens.append(word[:2])\n",
        "            word = word[2:]\n",
        "    else:  # where text is a string\n",
        "      word = ' '.join(list(text)) + ' </w>'\n",
        "      while word:\n",
        "        if word in self.vocab:\n",
        "          tokens.append(word)\n",
        "          break\n",
        "        else:\n",
        "          tokens.append(word[:2])\n",
        "          word = word[2:]\n",
        "    return tokens\n",
        "\n",
        "  def detokenize(self, tokens):\n",
        "    detokenized_txt = ''.join(tokens)\n",
        "    detokenized_txt = detokenized_txt.replace('</w>', '')\n",
        "    return detokenized_txt\n",
        "\n",
        "  def validate(self, val_data):\n",
        "    correct_tokens = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for text in val_data:\n",
        "\n",
        "      token_txt = self.tokenize(text)\n",
        "      detoken_txt = self.detokenize(token_txt)\n",
        "\n",
        "      if detoken_txt == text:\n",
        "        correct_tokens += 1\n",
        "\n",
        "      total_samples +=1\n",
        "\n",
        "    accuracy = correct_tokens / total_samples\n",
        "    print(f\"accuracy is {accuracy*100} %\")\n",
        "    return accuracy\n",
        "\n",
        "# training the tokenizer on the training data\n",
        "num_merges = 50\n",
        "tokenizer = SubwordTokenizer(num_merges)\n",
        "tokenizer.learn_bpe(bpe_train_data)\n",
        "\n",
        "# tokenized validation data\n",
        "tokenized_validation_data = [tokenizer.tokenize(sentence) for sentence in bpe_val_data]\n",
        "\n",
        "# detokenized val data\n",
        "detokenized_validation_data = [tokenizer.detokenize(tokens) for tokens in tokenized_validation_data]"
      ],
      "metadata": {
        "id": "G8tzESxE7xpH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**gpt code from here**"
      ],
      "metadata": {
        "id": "XwuQZkih8ulf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_train = timeit.default_timer()\n",
        "\n",
        "print(f\"time taken to fetch and write the data {token_train - start_time_2} secs\")"
      ],
      "metadata": {
        "id": "23uqFyYguiGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439cfe72-0323-4b0b-d876-83bbdcc10530"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken to fetch and write the data 58.077128619999996 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the data\n",
        "file_path = '/content/drive/MyDrive/new_training_data.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  data = file.read()\n",
        "total_no_of_words = len(data)\n",
        "print(total_no_of_words)"
      ],
      "metadata": {
        "id": "lcZMRIKmq8Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3e5986-7484-495c-f7b6-f34272cf499d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "219382798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # cleaning the text data\n",
        "# import re\n",
        "# data = re.sub('[^a-zA-Z0-9\\s.,!?]', '', data)"
      ],
      "metadata": {
        "id": "-x10TJI8sLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total no of chars and vocab size\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "fy-eNUDesO6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f12f2b8-21bc-4f15-84ac-5c8aefeaf90f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡£­°²³´µ·¼½ÀÁÃÄÅÉÍÓÖ×ßàáâãäåæçèéêëìíîïñòóôõöøúüāăćČĐğīİŁłōśūŻʖʻʼ̴̵̶̷̸̡̢̧̨̛̖̗̘̙̜̝̞̟̠̣̤̥̦̩̪̫̬̭̮̯̰̱̲̳̹̺̻̼͇͈͉͍͎͓͔͕͖͙͚̀́̂̃̄̅̆̇̈̉̊̋̌̍̎̏̐̑̒̓̔̽̾̿̀́͂̓̈́͆͊͋͌͐͑͒͗͛̕̚͘͜͝͠͡ͅΧιρςστχόϑДಠ ​‍–—―‘’‚“”… ›‽⁠₂€₹™⅓⅔∆−≈⍵♩♪♫♬♭⚡✨。えァアウェオサシジスッデトナニビフブボメョリルロンー一万写動千夜如宇宏开忽春来树梨活真花萌风꞉️﻿，ａｄｅｇｈｉｍｎｒｔｕｖｙ�𝅘𝅥𝅮𝔻𝕄𝕌𝕐𝕒𝕓𝕔𝕕𝕖𝕗𝕘𝕙𝕚𝕛𝕜𝕝𝕞𝕟𝕠𝕡𝕢𝕣𝕤𝕥𝕦𝕧𝕨𝕪𝘐𝘢𝘣𝘤𝘥𝘦𝘧𝘩𝘪𝘭𝘮𝘯𝘰𝘱𝘳𝘴𝘵𝘶𝘸𝘺🔮🤔\n",
            "416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "NIh0X7Cspotc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the data seperately\n",
        "\n",
        "# normal tokenization first\n",
        "train_word = nltk.word_tokenize(train_data)\n",
        "val_data = nltk.word_tokenize(val_data)\n",
        "\n",
        "# sub-word tokenization\n",
        "train_data = tokenizer.tokenize(train_data)\n",
        "val_data = tokenizer.tokenize(val_data)"
      ],
      "metadata": {
        "id": "UxUwNAYioezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[:10])\n",
        "print(val_data[:10])"
      ],
      "metadata": {
        "id": "KuCqNmT9z5He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_time = timeit.default_timer()\n",
        "print(f\"time taken to fetch and write the data {(token_time - start_time) / 60} mins\")"
      ],
      "metadata": {
        "id": "P5dq73r3uyRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # independent sequences process in parallel\n",
        "block_size = 128 # maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1400)\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, num_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Linear(n_embd, 4 * n_embd))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(4 * n_embd, n_embd))\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# no of parameters in the model\n",
        "no_of_param = sum(p.numel() for p in m.parameters())\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "steps = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Store step and loss values for visualization\n",
        "        steps.append(iter)\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "torch.save(model.state_dict(), 'transformer_model_v3.pth')\n",
        "\n",
        "output_data = tokenizer.detokenize(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "print(output_data)"
      ],
      "metadata": {
        "id": "SRNw2nX3njOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train = timeit.default_timer()\n",
        "\n",
        "print(f\"time taken to train the model {(model_train - start_time) / 3600 } hrs\")"
      ],
      "metadata": {
        "id": "KRSkGZd8pbgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, train_losses, label='Train Loss')\n",
        "plt.plot(steps, val_losses, label='Validation Loss')\n",
        "plt.title('Loss Over Steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fvbzqc4Mvnrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "end_time = timeit.default_timer()\n",
        "\n",
        "total_time = (end_time - start_time_2) / 3600\n",
        "\n",
        "ffn_factor = 4\n",
        "embedding_params = n_embd * vocab_size\n",
        "attention_params = n_head * (n_embd // n_head * 2 * n_embd) * n_layer\n",
        "\n",
        "feedforward_params = n_embd * ffn_factor * n_layer * 2\n",
        "total_param = no_of_param / 1e6\n",
        "total_params = embedding_params + attention_params + feedforward_params\n",
        "\n",
        "print(\"///// summary /////\")\n",
        "print(f\"total time taken was {total_time} hrs\")\n",
        "# print(f\"time just to fetch the data was {(data_coll - start_time) / 3600} hrs and no of videos fetched were {videoNo}\")\n",
        "print(f\"total no of words in the file were: {total_no_of_words/1e6} million\")\n",
        "print(f\"total vocab size was {vocab_size}\")\n",
        "print(\"total no of calculated parameters:\", total_params)\n",
        "print(\"total no of actual parameters:\", total_param)\n",
        "print(f\"time taken to train the model was {(model_train - start_time_2) / 3600}\")\n",
        "print(f\"model ran for {max_iters} iterations and final val loss: {val_losses[-1]} and train loss: {train_losses[-1]}\")\n",
        "print('\\n', '\\n')\n",
        "print(\"//// generated output ////\")\n",
        "print(output_data)"
      ],
      "metadata": {
        "id": "DetNldUaw_Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKGmtIKHnFIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}