{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/SmallLanguageModel-project/blob/main/Demo%20Models/Colab%20Notebooks/MainGPT_KarpathyVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input_data.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.65630578994751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "oTo.JUZ!!zqe!\n",
            "xBP qbs$Gy'AcOmrLwwt\n",
            "p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\n",
            "rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\n",
            "ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\n",
            "SV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\n",
            "tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\n",
            "pSPYgCuCJrIFtb\n",
            "jQXg\n",
            "pA.P LP,SPJi\n",
            "DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\n",
            "D.?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('training_data.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "import re\n",
        "\n",
        "text = re.sub('[^a-zA-Z0-9\\s.,!?]', '', text)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "steps = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Store step and loss values for visualization\n",
        "        steps.append(iter)\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "torch.save(model.state_dict(), 'transformer_model.pth')\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "hoelkOrFY8bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f234603b-c951-4ba6-dfe8-d738bfd59e05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.212551 M parameters\n",
            "step 0: train loss 4.4477, val loss 4.4437\n",
            "step 100: train loss 2.5959, val loss 2.6049\n",
            "step 200: train loss 2.5009, val loss 2.5071\n",
            "step 300: train loss 2.4355, val loss 2.4469\n",
            "step 400: train loss 2.3799, val loss 2.3825\n",
            "step 500: train loss 2.3076, val loss 2.3118\n",
            "step 600: train loss 2.2311, val loss 2.2454\n",
            "step 700: train loss 2.1712, val loss 2.1777\n",
            "step 800: train loss 2.1109, val loss 2.1194\n",
            "step 900: train loss 2.0657, val loss 2.0694\n",
            "step 1000: train loss 2.0215, val loss 2.0354\n",
            "step 1100: train loss 1.9918, val loss 1.9984\n",
            "step 1200: train loss 1.9524, val loss 1.9646\n",
            "step 1300: train loss 1.9383, val loss 1.9441\n",
            "step 1400: train loss 1.9025, val loss 1.9166\n",
            "step 1500: train loss 1.8886, val loss 1.8971\n",
            "step 1600: train loss 1.8631, val loss 1.8784\n",
            "step 1700: train loss 1.8478, val loss 1.8573\n",
            "step 1800: train loss 1.8288, val loss 1.8396\n",
            "step 1900: train loss 1.8231, val loss 1.8396\n",
            "step 2000: train loss 1.8050, val loss 1.8239\n",
            "step 2100: train loss 1.7912, val loss 1.8029\n",
            "step 2200: train loss 1.7876, val loss 1.8027\n",
            "step 2300: train loss 1.7640, val loss 1.7782\n",
            "step 2400: train loss 1.7608, val loss 1.7738\n",
            "step 2500: train loss 1.7458, val loss 1.7603\n",
            "step 2600: train loss 1.7360, val loss 1.7575\n",
            "step 2700: train loss 1.7247, val loss 1.7470\n",
            "step 2800: train loss 1.7191, val loss 1.7399\n",
            "step 2900: train loss 1.7066, val loss 1.7271\n",
            "step 3000: train loss 1.6919, val loss 1.7184\n",
            "step 3100: train loss 1.6923, val loss 1.7193\n",
            "step 3200: train loss 1.6816, val loss 1.7026\n",
            "step 3300: train loss 1.6754, val loss 1.7046\n",
            "step 3400: train loss 1.6782, val loss 1.6972\n",
            "step 3500: train loss 1.6782, val loss 1.6972\n",
            "step 3600: train loss 1.6619, val loss 1.6789\n",
            "step 3700: train loss 1.6604, val loss 1.6820\n",
            "step 3800: train loss 1.6555, val loss 1.6822\n",
            "step 3900: train loss 1.6474, val loss 1.6676\n",
            "step 4000: train loss 1.6425, val loss 1.6694\n",
            "step 4100: train loss 1.6400, val loss 1.6652\n",
            "step 4200: train loss 1.6412, val loss 1.6659\n",
            "step 4300: train loss 1.6334, val loss 1.6587\n",
            "step 4400: train loss 1.6251, val loss 1.6526\n",
            "step 4500: train loss 1.6216, val loss 1.6577\n",
            "step 4600: train loss 1.6211, val loss 1.6393\n",
            "step 4700: train loss 1.6192, val loss 1.6381\n",
            "step 4800: train loss 1.6149, val loss 1.6434\n",
            "step 4900: train loss 1.6084, val loss 1.6333\n",
            "step 4999: train loss 1.5983, val loss 1.6365\n",
            "\n",
            "would\n",
            "get combletchip answer from flyissmanated by wehindated to maybe to service of moode in dot voicity\n",
            "nice could look know. Bit patter shalast corres and and you roomerching heath, s\n",
            "comber one lets and if they computers, its witasted who cator casitically friend and their handoped about audocously\n",
            "zerodys, corning. Which \n",
            "is out theres kind with killing of in just feel election \n",
            "need from have on Could all gow 0 drivo to divice intodges the tagenot the Grapheasese are elected blust changed fill years according\n",
            "Phiidy gengtith and new typices. Their\n",
            "that montex have nere Eyembis reason thungs to a day singations that is it was the matter four bargan fingered in a for ane altex seeivets sking does, side and inereged with\n",
            "the Sceoned to in heard you guys picker of fined so in dater Senistly you, In nenother valage way you as it probemic on ingter\n",
            "four most to reminaxis. Kometures airson. So is deeps if fooday  in tell glneves leve cenorons instrading fluding Fr Randoi Four Betd, get Altaridered are decoments tather Reed sing while you defore Fob, it bit monthing and abovity hydelt\n",
            "basel it out to most of the sladers a bit unning thatibility this dreaming history increctives at the deementather, are waeky notes. But they about\n",
            "fitcidentiations. Leates and Eurotherio, toh like, huved five at 1812,\n",
            "I differentions infludencemature Mush had to stop about sour were dont sell want conferpanding\n",
            "from up to many of mativite asa and last its nations for a quices areas are of what it was most first found These 2201 orbutput their are that sale with, a different outside Ph PC Andy to works and that throuts is vapting you can pass, you to hannt about these would see belier incream went chroas, isnt\n",
            "of the video underway, for 8, whats firster is like it Conentinal Septh\n",
            "put, dequition olds. Zopite Carles impulled on labord on sevellevement usually at own the whord that gover arounded treen they to makes that miral technology work Citin. Amprocrave. Are befuild any other to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, train_losses, label='Train Loss')\n",
        "plt.plot(steps, val_losses, label='Validation Loss')\n",
        "plt.title('Loss Over Steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "DI7vQuUvQVRh",
        "outputId": "0b77ad35-eeab-46db-9427-d68fd2291a3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKnLo1roSegJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}